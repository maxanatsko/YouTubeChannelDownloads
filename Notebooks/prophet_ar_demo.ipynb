{"cells":[{"cell_type":"code","source":["# Prophet Account Receivable Forecasting Demo\n","# MS Fabric Lakehouse Notebook\n","\n","# Install Prophet (run this cell first)\n","%pip install prophet\n","\n","import pandas as pd\n","import numpy as np\n","from prophet import Prophet\n","import matplotlib.pyplot as plt\n","from datetime import datetime, timedelta\n","import warnings\n","warnings.filterwarnings('ignore')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0203aa7-c652-4996-b68b-a878cbce5e5a"},{"cell_type":"code","source":["# =============================================================================\n","# 1. CREATE SAMPLE AR DATA\n","# =============================================================================\n","\n","# Generate sample AR data - replace this section with your actual data loading\n","np.random.seed(43)\n","start_date = datetime(2020, 1, 1)\n","end_date = datetime(2024, 12, 31)\n","date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n","\n","# Create realistic AR patterns\n","sample_data = []\n","for date in date_range:\n","    base_amount = 50000  # Base daily collections\n","    \n","    # Add seasonality patterns\n","    month_effect = 1.2 if date.month in [12, 1, 3, 6] else 1.0  # Quarter-end spikes\n","    day_effect = 0.7 if date.weekday() >= 5 else 1.0  # Lower weekend collections\n","    \n","    # Add trend\n","    years_since_start = (date - start_date).days / 365.25\n","    trend_effect = 1 + (years_since_start * 0.05)  # 5% annual growth\n","    \n","    # Add some randomness\n","    noise = np.random.normal(1, 0.15)\n","    \n","    daily_collections = base_amount * month_effect * day_effect * trend_effect * noise\n","    daily_collections = max(daily_collections, 0)  # No negative collections\n","    \n","    sample_data.append({\n","        'date': date,\n","        'collections': daily_collections,\n","        'customer_segment': np.random.choice(['Enterprise', 'SMB', 'Government'], p=[0.5, 0.3, 0.2])\n","    })\n","\n","df_raw = pd.DataFrame(sample_data)\n","\n","# Aggregate to monthly data\n","df_monthly = df_raw.groupby([df_raw['date'].dt.to_period('M')]).agg({\n","    'collections': 'sum'\n","}).reset_index()\n","df_monthly['date'] = df_monthly['date'].dt.to_timestamp(how='end') #transform to month-end dates\n","\n","print(\"Sample AR Data (Monthly Collections):\")\n","print(df_monthly.head(10))\n","print(f\"\\nData shape: {df_monthly.shape}\")\n","print(f\"Date range: {df_monthly['date'].min()} to {df_monthly['date'].max()}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"2259077f-893d-4461-b997-089da93fd64a"},{"cell_type":"code","source":["# =============================================================================\n","# 1.5 WRITE SAMPLE AR DATA TO LAKEHOUSE\n","# =============================================================================\n","\n","# Convert to Spark DataFrame\n","df_monthly_spark = spark.createDataFrame(df_monthly)\n","\n","# Write to Delta table in your Lakehouse\n","df_monthly_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ar_collections_monthly\")\n","\n","print(\"✓ Data uploaded to Lakehouse table: ar_collections_monthly\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"a41d5403-acfa-45f5-b403-4f4899b25110"},{"cell_type":"code","source":["# =============================================================================\n","# 2. LOAD AR DATA FROM LAKEHOUSE\n","# =============================================================================\n","\n","\"\"\"\n","# Load from Lakehouse table\n","df_ar = spark.sql('''\n","    SELECT \n","        date as date,\n","        SUM(collections) as collections\n","    FROM Prophet_Lakehouse.dbo.ar_collections_monthly\n","    WHERE date >= '2020-01-01'\n","    GROUP BY date\n","    ORDER BY date\n","''').toPandas()\n","\"\"\"\n","\n","# Load from Delta table\n","df_ar = spark.read.format(\"delta\").load(\"Tables/dbo/ar_collections_monthly\").toPandas()\n","\n","# Count rows (use len() for pandas DataFrame)\n","row_count = len(df_ar)\n","\n","print(f\"✓ Data loaded: {row_count} rows\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ab2ee2af-85f8-4b22-bf9c-afd6d9144e0e"},{"cell_type":"code","source":["# =============================================================================\n","# 3. PREPARE DATA FOR PROPHET\n","# =============================================================================\n","\n","# Prophet requires specific column names: 'ds' (datestamp) and 'y' (value)\n","#prophet_data = df_ar[['date', 'collections']].copy()\n","#prophet_data.columns = ['ds', 'y']\n","\n","\n","df0 = df_ar.copy()\n","ds = (pd.to_datetime(df0['date'], utc=True)\n","        .dt.tz_localize(None)\n","        .astype('datetime64[ns]'))\n","\n","prophet_data = (pd.DataFrame({'ds': ds, 'y': df0['collections']})\n","                  .set_index('ds')\n","                  .resample('M')['y'].sum()\n","                  .rename_axis('ds').reset_index()\n","                  .sort_values('ds').reset_index(drop=True))\n","\n","# Asserts\n","assert pd.infer_freq(prophet_data['ds']) == 'M'\n","assert prophet_data['ds'].dt.is_month_end.all()\n","\n","# Remove any missing values\n","prophet_data = prophet_data.dropna()\n","\n","print(\"\\nData prepared for Prophet:\")\n","print(prophet_data.head())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ca45a6c3-7f83-40f5-a12e-1091f8d47593"},{"cell_type":"code","source":["# =============================================================================\n","# 4. INITIALIZE PROPHET MODEL\n","# =============================================================================\n","\n","# Initialize Prophet model\n","model = Prophet(\n","    # ==================== TREND PARAMETERS ====================\n","    growth='linear',  # 'linear' or 'logistic'\n","                      # linear: Unlimited growth (good for most business metrics)\n","                      # logistic: Growth with saturation limit (set cap/floor)\n","                      # TUNE: If actuals show clear ceiling/floor, switch to logistic\n","    \n","    changepoint_prior_scale=0.05,  # Range: 0.001 to 0.5+ (default: 0.05)\n","                                   # Controls trend flexibility/changepoint sensitivity\n","                                   # Lower = smoother, fewer trend changes\n","                                   # Higher = more reactive to shifts\n","                                   # TUNE BY AR: \n","                                   #   - Forecast misses level shifts → INCREASE (try 0.1, 0.5, 1.0)\n","                                   #   - Forecast too jagged/overfits → DECREASE (try 0.01, 0.001)\n","                                   #   - Systematic over/under at trend breaks → INCREASE\n","    \n","    n_changepoints=25,  # Number of potential changepoints (default: 25)\n","                        # More points = can detect more trend shifts\n","                        # TUNE BY AR:\n","                        #   - Multiple missed level changes → INCREASE to 50-100\n","                        #   - Overfitting with erratic changes → DECREASE to 10-15\n","    \n","    changepoint_range=0.95,  # Fraction of history where changepoints allowed (default: 0.8)\n","                             # 0.8 = only first 80% of training data\n","                             # 0.95 = allows trend changes closer to forecast period\n","                             # TUNE BY AR:\n","                             #   - Forecast misses recent shift → INCREASE to 0.9-0.95\n","                             #   - Helps when level changed right before forecast period\n","    \n","    # ==================== SEASONALITY PARAMETERS ====================\n","    yearly_seasonality=True,  # Can also be integer (Fourier terms)\n","                              # True = auto (10 terms), False = none\n","                              # Integer (e.g., 10, 20) = custom complexity\n","                              # TUNE BY AR:\n","                              #   - Seasonal pattern too smooth → INCREASE to 15-20\n","                              #   - Seasonal pattern too wiggly → DECREASE to 5-8\n","    \n","    weekly_seasonality=False,  # Same logic as yearly\n","                               # Only relevant if data is daily/hourly\n","                               # True = 3 Fourier terms\n","    \n","    daily_seasonality=False,   # Only for hourly/minute data\n","                               # True = 4 Fourier terms\n","    \n","    seasonality_prior_scale=10.0,  # Range: 0.01 to 100+ (default: 10.0)\n","                                    # Controls seasonality strength/amplitude\n","                                    # Lower = weaker seasonal effect\n","                                    # Higher = stronger seasonal peaks/troughs\n","                                    # TUNE BY AR:\n","                                    #   - Forecast peaks too flat → INCREASE (try 15, 20, 30)\n","                                    #   - Forecast peaks too extreme → DECREASE (try 5, 1, 0.1)\n","                                    #   - Check residuals: if seasonal pattern remains → INCREASE\n","    \n","    seasonality_mode='additive',  # 'additive' or 'multiplicative'\n","                                   # additive: Seasonal variation stays constant (+/- same amount)\n","                                   # multiplicative: Variation grows with level (percentage-based)\n","                                   # TUNE BY AR:\n","                                   #   - Seasonal swings grow over time → use 'multiplicative'\n","                                   #   - Seasonal swings constant → use 'additive'\n","                                   #   - Visual check: does % variation stay similar? → multiplicative\n","    \n","    # ==================== UNCERTAINTY PARAMETERS ====================\n","    interval_width=0.95,  # Confidence interval width (default: 0.80)\n","                          # 0.80 = 80% interval, 0.95 = 95% interval\n","                          # Wider = more conservative bounds\n","                          # TUNE BY AR:\n","                          #   - Too many actuals outside bands → INCREASE to 0.95\n","                          #   - Bands too wide to be useful → DECREASE to 0.60-0.70\n","                          #   - This doesn't affect forecast, only uncertainty bands\n","    \n","    mcmc_samples=300,  # Bayesian sampling for uncertainty (default: 0 = no MCMC)\n","                     # 0 = fast MAP estimation\n","                     # 300+ = slower but better uncertainty estimates\n","                     # TUNE: Use 300-500 if you need reliable prediction intervals\n","                     #       (doesn't change point forecast, only affects uncertainty)\n","    \n","    # ==================== OTHER USEFUL PARAMETERS ====================\n","    \n","    # holidays=holidays_df,  # DataFrame with holiday dates and effects\n","                              # Columns: ['ds', 'holiday', 'lower_window', 'upper_window']\n","                              # TUNE BY AR:\n","                              #   - Spikes on specific dates → add those dates as holidays\n","                              #   - Check residuals for recurring anomalies\n","    \n","    # Add custom seasonalities for non-standard periods:\n","    # model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n","    # model.add_seasonality(name='quarterly', period=91.25, fourier_order=8)\n","    # TUNE BY AR: If you see patterns at specific frequencies\n","    \n","    # Add regressors (external variables):\n","    # model.add_regressor('promo', prior_scale=0.5, mode='additive')\n","    # TUNE BY AR: If actuals correlate with external events/variables\n",")\n","\n","# ==================== ADVANCED TUNING BASED ON AR ANALYSIS ====================\n","\n","# After fitting, analyze residuals:\n","# residuals = actual - forecast['yhat']\n","# \n","# PATTERN → ACTION:\n","# \n","# 1. Systematic over/under prediction:\n","#    - Early period: Increase changepoint_range\n","#    - Recent period: Check if training data includes recent level\n","#    - Throughout: Increase changepoint_prior_scale\n","#\n","# 2. Missed seasonal peaks:\n","#    - Increase seasonality_prior_scale\n","#    - Try seasonality_mode='multiplicative'\n","#    - Increase fourier_order for yearly_seasonality\n","#\n","# 3. Forecast too smooth:\n","#    - Increase changepoint_prior_scale (0.1 → 0.5 → 1.0)\n","#    - Increase seasonality_prior_scale (10 → 20 → 30)\n","#\n","# 4. Forecast too volatile/overfit:\n","#    - Decrease changepoint_prior_scale (0.05 → 0.01)\n","#    - Decrease seasonality_prior_scale (10 → 5 → 1)\n","#    - Reduce n_changepoints\n","\n","\"\"\"\n","# Add custom holidays/events (e.g., fiscal year end, major customer payments)\n","holidays = pd.DataFrame({\n","    'holiday': 'fiscal_year_end',\n","    'ds': pd.to_datetime(['2021-12-31', '2022-12-31', '2023-12-31', '2024-12-31', '2025-12-31']),\n","    'lower_window': -5,\n","    'upper_window': 5,\n","})\n","\"\"\"\n","\n","\"\"\"\n","# Add external regressors (if you have economic indicators, etc.)\n","\n","# Example: Adding GDP growth as regressor\n","prophet_data['gdp_growth'] = your_gdp_data  # Add your external data\n","future_advanced['gdp_growth'] = future_gdp_data  # Add future values\n","\n","model_with_regressors = Prophet()\n","model_with_regressors.add_regressor('gdp_growth')\n","model_with_regressors.fit(prophet_data)\n","\"\"\"\n","\n","\n","# Fit the model\n","print(\"\\nFitting Prophet model...\")\n","model.fit(prophet_data)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dbbc9bd2-540b-4bdf-9228-23e52c895a8d"},{"cell_type":"code","source":["# =============================================================================\n","# 5. CREATE FUTURE PREDICTIONS\n","# =============================================================================\n","\n","# Create future dataframe for next 12 months\n","future = model.make_future_dataframe(periods=12, freq='M')\n","print(f\"\\nPredicting {12} months ahead...\")\n","\n","# Make predictions\n","forecast = model.predict(future)\n","\n","# Display forecast results\n","forecast_summary = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12)\n","forecast_summary.columns = ['Date', 'Predicted_Collections', 'Lower_Bound', 'Upper_Bound']\n","print(\"\\nNext 12 months forecast:\")\n","print(forecast_summary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9c18602-4446-4894-9c5c-bbd3e4fc404e"},{"cell_type":"code","source":["# =============================================================================\n","# 6. VISUALIZATIONS\n","# =============================================================================\n","\n","# Plot forecast\n","fig1 = model.plot(forecast, figsize=(12, 6))\n","plt.title('Account Receivable Collections Forecast')\n","plt.xlabel('Date')\n","plt.ylabel('Collections ($)')\n","plt.show()\n","\n","# Plot components (trend, seasonality)\n","fig2 = model.plot_components(forecast, figsize=(12, 8))\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5816af11-50ae-4ae1-95f7-ae178a48fdba"},{"cell_type":"code","source":["# =============================================================================\n","# 7. SAVE RESULTS TO LAKEHOUSE\n","# =============================================================================\n","\n","# Prepare forecast data for saving\n","forecast_final = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()\n","forecast_final.columns = ['forecast_date', 'predicted_collections', 'lower_bound', 'upper_bound']\n","forecast_final['model_run_date'] = datetime.now()\n","forecast_final['model_type'] = 'Prophet'\n","\n","# Convert to Spark DataFrame and save to Lakehouse\n","forecast_spark = spark.createDataFrame(forecast_final)\n","\n","# Save to Delta table\n","forecast_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Prophet_Lakehouse.dbo.ar_forecast\")\n","print(\"Forecast saved to Lakehouse table: Prophet_Lakehouse.dbo.ar_forecast\")\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"AR FORECASTING COMPLETE\")\n","print(\"=\"*50)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"43c996b4-8907-427f-bfe8-2d4efdc4e3c6"},{"cell_type":"code","source":["# =============================================================================\n","# MODEL BACK-TESTING AND CROSS-VALIDATION\n","# =============================================================================\n","\n","import numpy as np\n","import pandas as pd\n","from prophet import Prophet\n","from prophet.diagnostics import cross_validation, performance_metrics\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from itertools import product\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# 1. DEFINE PARAMETER GRID\n","# =============================================================================\n","\n","param_grid = {\n","    'changepoint_prior_scale': [0.05, 0.1, 0.5],\n","    'seasonality_prior_scale': [10.0, 20.0, 30.0],\n","    'seasonality_mode': ['additive', 'multiplicative'],\n","    'changepoint_range': [0.9, 0.95], \n","}\n","\n","# Fixed parameters (not tuned)\n","base_params = {\n","    'growth': 'linear',\n","    'yearly_seasonality': True,\n","    'weekly_seasonality': False,\n","    'daily_seasonality': False,\n","    'n_changepoints': 25,\n","    'interval_width': 0.80,\n","}\n","\n","# =============================================================================\n","# 2. CROSS-VALIDATION FUNCTION\n","# =============================================================================\n","\n","def evaluate_params(params_dict, data, cv_config):\n","    \"\"\"\n","    Evaluate a parameter set using time-series cross-validation\n","    \n","    Args:\n","        params_dict: Dictionary of Prophet parameters\n","        data: Training data (DataFrame with 'ds' and 'y')\n","        cv_config: Cross-validation configuration\n","    \n","    Returns:\n","        Dictionary with performance metrics\n","    \"\"\"\n","    try:\n","        # Combine base params with test params\n","        full_params = {**base_params, **params_dict}\n","        \n","        # Train model\n","        model = Prophet(**full_params)\n","        model.fit(data)\n","        \n","        # Cross-validation\n","        df_cv = cross_validation(\n","            model,\n","            initial=cv_config['initial'],\n","            period=cv_config['period'],\n","            horizon=cv_config['horizon'],\n","            parallel=\"processes\"  # Speed up with parallel processing\n","        )\n","        \n","        # Calculate metrics\n","        df_metrics = performance_metrics(df_cv)\n","        \n","        return {\n","            'params': params_dict,\n","            'mape': df_metrics['mape'].mean(),\n","            'mae': df_metrics['mae'].mean(),\n","            'rmse': df_metrics['rmse'].mean(),\n","            'coverage': df_metrics['coverage'].mean(),  # % of actuals in prediction interval\n","            'df_cv': df_cv,\n","            'df_metrics': df_metrics\n","        }\n","    \n","    except Exception as e:\n","        print(f\"Error with params {params_dict}: {str(e)}\")\n","        return {\n","            'params': params_dict,\n","            'mape': np.inf,\n","            'mae': np.inf,\n","            'rmse': np.inf,\n","            'coverage': 0,\n","            'error': str(e)\n","        }\n","\n","# =============================================================================\n","# 3. GRID SEARCH WITH CROSS-VALIDATION\n","# =============================================================================\n","\n","# Cross-validation configuration\n","cv_config = {\n","    'initial': '730 days',   # Minimum training data (2 years)\n","    'period': '180 days',    # Step between cutoff dates (6 months)\n","    'horizon': '365 days',   # Forecast horizon to evaluate (1 year)\n","}\n","\n","print(\"Starting Grid Search with Cross-Validation...\")\n","print(f\"CV Config: {cv_config}\")\n","print(f\"Testing {np.prod([len(v) for v in param_grid.values()])} parameter combinations\\n\")\n","\n","# Generate all parameter combinations\n","keys = param_grid.keys()\n","values = param_grid.values()\n","param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","\n","# Evaluate each combination\n","results = []\n","for i, params in enumerate(param_combinations, 1):\n","    print(f\"Testing {i}/{len(param_combinations)}: {params}\")\n","    result = evaluate_params(params, prophet_data, cv_config)\n","    results.append(result)\n","    print(f\"  MAPE: {result['mape']:.2f}%, MAE: {result['mae']:.2f}, RMSE: {result['rmse']:.2f}\\n\")\n","\n","# =============================================================================\n","# 4. ANALYZE RESULTS\n","# =============================================================================\n","\n","# Convert to DataFrame\n","results_df = pd.DataFrame([\n","    {\n","        **r['params'],\n","        'mape': r['mape'],\n","        'mae': r['mae'],\n","        'rmse': r['rmse'],\n","        'coverage': r['coverage']\n","    }\n","    for r in results if r['mape'] != np.inf\n","])\n","\n","# Sort by MAPE\n","results_df = results_df.sort_values('mape').reset_index(drop=True)\n","\n","# Display top 10 models\n","print(\"\\n\" + \"=\"*80)\n","print(\"TOP 10 MODELS BY MAPE\")\n","print(\"=\"*80)\n","print(results_df.head(10).to_string(index=False))\n","print(\"\\n\")\n","\n","# Best model\n","best_params = results_df.iloc[0].to_dict()\n","print(\"=\"*80)\n","print(\"BEST MODEL PARAMETERS\")\n","print(\"=\"*80)\n","for key, value in best_params.items():\n","    print(f\"{key:30s}: {value}\")\n","print(\"\\n\")\n","\n","# =============================================================================\n","# 5. VISUALIZE RESULTS\n","# =============================================================================\n","\n","fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n","\n","# Plot 1: MAPE vs changepoint_prior_scale\n","ax1 = axes[0, 0]\n","for mode in results_df['seasonality_mode'].unique():\n","    subset = results_df[results_df['seasonality_mode'] == mode]\n","    ax1.scatter(subset['changepoint_prior_scale'], subset['mape'], \n","               label=mode, alpha=0.6, s=100)\n","ax1.set_xlabel('Changepoint Prior Scale', fontsize=12)\n","ax1.set_ylabel('MAPE (%)', fontsize=12)\n","ax1.set_xscale('log')\n","ax1.legend()\n","ax1.set_title('MAPE vs Changepoint Prior Scale', fontsize=14, fontweight='bold')\n","ax1.grid(True, alpha=0.3)\n","\n","# Plot 2: MAPE vs seasonality_prior_scale\n","ax2 = axes[0, 1]\n","for mode in results_df['seasonality_mode'].unique():\n","    subset = results_df[results_df['seasonality_mode'] == mode]\n","    ax2.scatter(subset['seasonality_prior_scale'], subset['mape'], \n","               label=mode, alpha=0.6, s=100)\n","ax2.set_xlabel('Seasonality Prior Scale', fontsize=12)\n","ax2.set_ylabel('MAPE (%)', fontsize=12)\n","ax2.set_xscale('log')\n","ax2.legend()\n","ax2.set_title('MAPE vs Seasonality Prior Scale', fontsize=14, fontweight='bold')\n","ax2.grid(True, alpha=0.3)\n","\n","# Plot 3: MAPE distribution by seasonality mode\n","ax3 = axes[1, 0]\n","results_df.boxplot(column='mape', by='seasonality_mode', ax=ax3)\n","ax3.set_xlabel('Seasonality Mode', fontsize=12)\n","ax3.set_ylabel('MAPE (%)', fontsize=12)\n","ax3.set_title('MAPE Distribution by Seasonality Mode', fontsize=14, fontweight='bold')\n","plt.sca(ax3)\n","plt.xticks(rotation=0)\n","\n","# Plot 4: Top 15 models comparison\n","ax4 = axes[1, 1]\n","top_15 = results_df.head(15)\n","x_pos = np.arange(len(top_15))\n","ax4.barh(x_pos, top_15['mape'], color='steelblue', alpha=0.7)\n","ax4.set_yticks(x_pos)\n","ax4.set_yticklabels([f\"Model {i+1}\" for i in range(len(top_15))], fontsize=9)\n","ax4.set_xlabel('MAPE (%)', fontsize=12)\n","ax4.set_title('Top 15 Models by MAPE', fontsize=14, fontweight='bold')\n","ax4.invert_yaxis()\n","ax4.grid(True, alpha=0.3, axis='x')\n","\n","plt.tight_layout()\n","plt.savefig('prophet_tuning_results.png', dpi=300, bbox_inches='tight')\n","plt.show()\n","\n","# =============================================================================\n","# 6. DETAILED CROSS-VALIDATION PLOT FOR BEST MODEL\n","# =============================================================================\n","\n","# Get best model's CV results\n","# Extract only parameter columns from best_params\n","param_columns = list(param_grid.keys())\n","best_params_only = {k: best_params[k] for k in param_columns}\n","\n","# Find matching result\n","best_result = [r for r in results if r['params'] == best_params_only][0]\n","df_cv_best = best_result['df_cv']\n","\n","# Plot CV results\n","fig, ax = plt.subplots(figsize=(15, 6))\n","\n","# Plot actual values\n","ax.scatter(df_cv_best['ds'], df_cv_best['y'], \n","          color='black', s=30, label='Actual', zorder=3)\n","\n","# Plot predictions\n","ax.scatter(df_cv_best['ds'], df_cv_best['yhat'], \n","          color='steelblue', s=20, alpha=0.5, label='Predicted', zorder=2)\n","\n","# Plot prediction intervals\n","ax.fill_between(df_cv_best['ds'], \n","                df_cv_best['yhat_lower'], \n","                df_cv_best['yhat_upper'],\n","                alpha=0.2, color='steelblue', label='80% Interval')\n","\n","ax.set_xlabel('Date', fontsize=12)\n","ax.set_ylabel('Collections ($)', fontsize=12)\n","ax.set_title('Cross-Validation Results - Best Model', fontsize=14, fontweight='bold')\n","ax.legend(loc='best')\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig('prophet_cv_best_model.png', dpi=300, bbox_inches='tight')\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8b9075c7-aa65-4fd3-afad-40c3976ce961"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"81773eda-9790-4ce0-960e-2374cc09aa01"}],"default_lakehouse":"81773eda-9790-4ce0-960e-2374cc09aa01","default_lakehouse_name":"Prophet_Lakehouse","default_lakehouse_workspace_id":"2876f9e7-75fe-40b7-9a61-83d1bbf5c52c"}}},"nbformat":4,"nbformat_minor":5}
